data:
  dataset_name: imdb
  file_path: data/dummy/pg19.csv  # placeholder, replace with tiny imdb csv if available
  tokenizer_name: bert-base-uncased
  max_length: 128
  text_col: Overview
  label_col: Genre
  task: classification

model:
  vocab_size: 30522
  embed_dim: 64
  num_classes: 5  # minimal subset
  num_layers: 2
  dropout: 0.1

training:
  batch_size: 8
  lr: 1e-3
  epochs: 2
  grad_clip: 1.0
  log_interval: 10 