data:
  dataset_name: nlp
  file_path: data/sample_text_classification.csv  # CSV/TSV with 'text' and 'label' columns
  tokenizer_name: bert-base-uncased
  max_length: 8
  split_frac:
    train: 0.8
    val: 0.1
    test: 0.1

model:
  input_dim: 1
  vocab_size: 30522  # for BERT tokenizer
  embed_dim: 32
  n_layers: 1
  dropout: 0.1
  num_classes: 2  # binary classification

training:
  batch_size: 8
  lr: 1e-4
  epochs: 2
  grad_clip: 1.0
  log_interval: 10 