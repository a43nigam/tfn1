data:
  dataset_name: "wikitext"
  wikitext_dataset_name: "wikitext-2-raw-v1"  # Use smaller dataset
  tokenizer_name: "gpt2"
  max_length: 128  # Reduced from 256 for memory efficiency
  text_col: "text"
  max_samples: 2000  # Reduced from 5000 for faster training

model:
  vocab_size: 50257
  embed_dim: 64   # Reduced from 128
  num_layers: 1   # Reduced from 2
  grid_size: 64   # Reduced from 128
  kernel_type: "rbf"
  evolution_type: "diffusion"
  interference_type: "standard"  # Required parameter for enhanced TFN
  dropout: 0.1
  num_heads: 4    # Reduced from 8
  max_seq_len: 128  # Match data max_length

training:
  batch_size: 4   # Reduced from 8
  lr: 1e-4
  epochs: 5       # Reduced from 10 for faster demo
  weight_decay: 0.01
  optimizer: "adamw"
  warmup_epochs: 1
  grad_clip: 1.0
  log_interval: 25 