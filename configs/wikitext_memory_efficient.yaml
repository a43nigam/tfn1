data:
  dataset_name: "wikitext"
  wikitext_dataset_name: "wikitext-2-raw-v1"  # Use smaller dataset
  tokenizer_name: "gpt2"
  max_length: 256  # Reduced from 512
  text_col: "text"
  max_samples: 5000  # Limit samples for memory efficiency

model:
  vocab_size: 50257
  embed_dim: 128  # Reduced from 256
  num_layers: 2   # Reduced from 4
  grid_size: 128  # Reduced from 256
  kernel_type: "rbf"
  evolution_type: "diffusion"
  dropout: 0.1

training:
  batch_size: 8   # Reduced from 32
  lr: 1e-4
  epochs: 10      # Reduced from 50
  weight_decay: 0.01
  optimizer: "adamw"
  warmup_epochs: 2
  grad_clip: 1.0
  log_interval: 50 