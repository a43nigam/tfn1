model_name: tfn_language_model

data:
  dataset_name: "wikitext"
  wikitext_dataset_name: "wikitext-2-raw-v1"  # Use smaller dataset
  tokenizer_name: "gpt2"
  max_length: 128  # Reduced from 256 for memory efficiency
  text_col: "text"
  max_samples: 2000  # Reduced from 5000 for faster training

model:
  vocab_size: 50257
  embed_dim: 64   # Reduced from 128
  grid_size: 64   # Reduced from 128
  kernel_type: "rbf"
  evolution_type: "pde"
  time_steps: 3
  dropout: 0.1

training:
  batch_size: 4   # Reduced from 8
  lr: 1e-4
  epochs: 5       # Reduced from 10 for faster demo
  weight_decay: 0.01
  optimizer: "adamw"
  warmup_epochs: 1
  grad_clip: 1.0
  log_interval: 25 