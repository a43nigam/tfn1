data:
  dataset_name: "wikitext"
  wikitext_dataset_name: "wikitext-2-raw-v1"
  tokenizer_name: "gpt2"
  max_length: 128
  text_col: "text"

model:
  vocab_size: 50257
  embed_dim: 128
  num_layers: 2
  grid_size: 128
  kernel_type: "rbf"
  evolution_type: "diffusion"

  dropout: 0.1

training:
  batch_size: 16
  lr: 1e-3
  epochs: 5
  weight_decay: 0.01
  optimizer: "adamw"
  warmup_epochs: 1
  grad_clip: 1.0
  log_interval: 50 