data:
  dataset_name: "wikitext"
  wikitext_dataset_name: "wikitext-2-raw-v1"  # or "wikitext-103-raw-v1" for larger dataset
  tokenizer_name: "gpt2"
  max_length: 512
  text_col: "text"

model:
  vocab_size: 50257  # GPT-2 vocabulary size
  embed_dim: 256
  num_layers: 4
  grid_size: 256
  kernel_type: "rbf"
  evolution_type: "diffusion"

  dropout: 0.1

training:
  batch_size: 32
  lr: 1e-4
  epochs: 50
  weight_decay: 0.01
  optimizer: "adamw"
  warmup_epochs: 5
  grad_clip: 1.0
  log_interval: 100 